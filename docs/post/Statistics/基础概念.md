创建于 2023-07-05<br>
关键词: 统计学基础术语.

## **随机变量**

随机变量是一种数学概念，它是一个函数，将样本空间中的每个可能的结果映射到实数轴上的一个值。简单来说，**随机变量是一种将随机事件的结果量化为数值的方法**。

例如，在抛硬币的随机过程中，我们可以定义一个随机变量X，它将正面朝上的结果映射为1，将反面朝上的结果映射为0。在掷骰子的随机过程中，我们可以定义一个随机变量Y，它将骰子落在1到6之间的任意一个数字映射为该数字对应的值。

随机变量通常用大写字母表示，如X、Y、Z等。它们可以是离散的或连续的，具体取决于它们所表示的随机过程。

## **随机过程**

随机过程是一个随机变量的集合。它是一族依赖于参数（通常是时间）的随机变量。每个随机变量代表了在某个特定时刻的状态。这些随机变量的集合构成了一个随机过程。

## **独立同分布**

在[概率论](https://zh.wikipedia.org/wiki/概率论)与[统计学](https://zh.wikipedia.org/wiki/统计学)中，**独立同分布**（英語：**Independent and identically distributed**，或稱獨立同分配，缩写为iid、 i.i.d.、IID）是指一组[随机变量](https://zh.wikipedia.org/wiki/随机变量)中每个变量的[概率分布](https://zh.wikipedia.org/wiki/概率分布)都相同，且这些随机变量互相[独立](https://zh.wikipedia.org/wiki/独立_(概率论))。

一组随机变量独立同分布并不意味着它们的[样本空间](https://zh.wikipedia.org/wiki/样本空间)中每个事件发生概率都相同。例如，投掷非均匀骰子得到的结果序列是独立同分布的，但掷出每个面朝上的概率并不相同。

## **中心极限定理**

**独立同分布的中心极限定理**，简单讲就是：均值为$\mu$，方差为$\sigma^2>0$的独立同分布的随机变量$X_1,X_2,\cdots X_n$的算术平均$\overline X=\frac{1}{n}\sum_{k=1}^nX_k$，当$n$充分大时近似地服从均值为$\mu$，方差为$\sigma^2/n$的正态分布。

已知如果一个随机变量服从正态分布，那么这个随机变量标准化处理后的衍生随机变量服从标准正态分布，因此根据上诉中心极限定理得知：当$n$充分大时，$\frac{\overline X-\mu}{\sigma/\sqrt n}$近似地服从标准正态分布。

## Cohen's h效应量

In [statistics](https://en.wikipedia.org/wiki/Statistics), **Cohen's h**, popularized by [Jacob Cohen](https://en.wikipedia.org/wiki/Jacob_Cohen_(statistician)), is a measure of distance between two proportions or [probabilities](https://en.wikipedia.org/wiki/Probability). Cohen's *h* has several related uses:

- It can be used to describe the difference between two proportions as "small", "medium", or "large".
- It can be used to determine if the difference between two proportions is "[meaningful](https://en.wikipedia.org/wiki/Clinical_significance)".
- It can be used in calculating the [sample size](https://en.wikipedia.org/wiki/Sample_size_determination) for a future study.

When measuring differences between proportions, Cohen's *h* can be used in conjunction with [hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing). A "[statistically significant](https://en.wikipedia.org/wiki/Statistical_significance)" difference between two proportions is understood to mean that, given the data, it is likely that there is a difference in the population proportions. However, this difference might be too small to be meaningful—the statistically significant result does not tell us the size of the difference. Cohen's *h*, on the other hand, quantifies the size of the difference, allowing us to decide if the difference is meaningful.

### Uses

Researchers have used Cohen's *h* as follows.

- Describe the differences in proportions using the [rule of thumb](https://en.wikipedia.org/wiki/Rule_of_thumb) criteria set out by Cohen. Namely, *h* = 0.2 is a "small" difference, *h* = 0.5 is a "medium" difference, and *h* = 0.8 is a "large" difference.
- Only discuss differences that have *h* greater than some threshold value, such as 0.2.
- When the sample size is so large that many differences are likely to be statistically significant, Cohen's *h* identifies "meaningful", "[clinically meaningful](https://en.wikipedia.org/wiki/Clinical_significance)", or "practically significant" differences.

### Calculation

Given a probability or proportion $p$, between $0$ and $1$, its [arcsine transformation](https://en.wikipedia.org/wiki/Arcsine_transformation) is
$$
2\arcsin \sqrt{p}
$$
Given two proportions, $p_1$ and $p_2$, $h$ is defined as the difference between their arcsine transformations. Namely,
$$
h = 2\left(\arcsin \sqrt{p_1} - \arcsin \sqrt{p_2}\right)
$$
This is also sometimes called "directional *h*" because, in addition to showing the magnitude of the difference, it shows which of the two proportions is greater.

Often, researchers mean "nondirectional *h*", which is just the absolute value of the directional *h*:
$$
h = \left|2\left(\arcsin \sqrt{p_1} - \arcsin \sqrt{p_2}\right)\right|
$$
